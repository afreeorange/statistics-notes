---
title: Bookmarks
---

## Sources of Knowledge

- Human Data
- Domain Knowledge
- Known Rules

---

* [Big Six Matrix Decompositions](https://nhigham.com/2022/05/18/the-big-six-matrix-factorizations/)
* [Probability and Statistics Cheatsheet](http://statistics.zone/)

## Multicollinearity and Collinearity

* https://towardsdatascience.com/statistics-in-python-collinearity-and-multicollinearity-4cc4dcd82b3f

## Kernel Trick

* https://gregorygundersen.com/blog/2019/12/10/kernel-trick/
* https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f#:~:text=The%20%E2%80%9Ctrick%E2%80%9D%20is%20that%20kernel,the%20data%20by%20these%20transformed
* https://towardsdatascience.com/understanding-the-kernel-trick-e0bc6112ef78
* https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/lectures/lec3.pdf
* https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d

## READ THIS

* https://sgfin.github.io/learning-resources/

## Courses

* [Foundations of Applied Mathematics](https://foundations-of-applied-mathematics.github.io/)

## Datasets

* [Statlib at CMU](http://lib.stat.cmu.edu/datasets/)
* [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)

## LaTeX / MathJAX Reference

* [A truly excellent resource, the only one I've ever needed](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)

## YouTube Channels

* [jbstatistics](https://www.youtube.com/channel/UCiHi6xXLzi9FMr9B0zgoHqA)  - Clear, short, precise
* [zedstatistics](https://www.youtube.com/channel/UC6AVa0vSrCpuskzGDDKz_EQ) - This man has saved my arse more times than I care to count

## Probability and Statistics

* [ISYE 6739 - Probability and Statistics](https://www2.isye.gatech.edu/~sman/courses/6739/) - Professor Goldsman's course.
* [The Algebra of Random Variables](https://en.wikipedia.org/wiki/Algebra_of_random_variables)

## Unfiled Bookmarks

* http://analystnotes.com/cfa-study-notes-types-of-multifactor-models.html
* http://essedunet.nsd.uib.no/cms/topics/regression/7/2.html
* http://www.cazaar.com/ta/econ113/interpreting-beta
* https://dynamicecology.wordpress.com/2015/02/05/how-many-terms-in-your-model-before-statistical-machismo/
* https://statisticsbyjim.com/hypothesis-testing/degrees-freedom-statistics/
* https://statisticsbyjim.com/regression/overfitting-regression-models/
* https://stats.idre.ucla.edu/stata/dae/logistic-regression/
* https://stats.stackexchange.com/questions/250912/how-to-say-if-the-variable-is-significant-looking-only-at-t-value
* https://stats.stackexchange.com/questions/277009/why-are-the-degrees-of-freedom-for-multiple-regression-n-k-1-for-linear-reg
* https://stats.stackexchange.com/questions/27724/do-all-interactions-terms-need-their-individual-terms-in-regression-model
* https://stats.stackexchange.com/questions/31/what-is-the-meaning-of-p-values-and-t-values-in-statistical-tests
* https://turi.com/learn/userguide/supervised-learning/random_forest_regression.html
* https://www.chegg.com/homework-help/questions-and-answers/adding-interaction-terms-decreases-model-complexity-leads-underfitting-increases-model-com-q35755900
* https://www.chegg.com/homework-help/questions-and-answers/following-likely-happen-long-term-excessive-investments-made-systematic-factor-funds-avera-q40964384
* https://www.chegg.com/homework-help/questions-and-answers/historically-value-premium-positive-2005-2012-average-annual-premium-value-stocks-212--tra-q40964337
* https://www.chegg.com/homework-help/questions-and-answers/linear-regression-model-add-categorical-variable-city-60-different-cities-leads-overfittin-q35756197
* https://www.factorresearch.com/research-value-and-momentum-factor-portfolios
* https://www.investopedia.com/terms/e/efficientmarkethypothesis.asp
* https://www.investopedia.com/terms/f/factor-investing.asp
* https://www.investopedia.com/terms/w/weakform.asp
* https://www.quora.com/What-is-the-difference-between-a-t-value-and-p-value


* [Everything you Should Know about Confusion Matrix for Machine Learning](https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/)

> Our model is saying “I can predict sick people 96% of the time”. However, it is doing the opposite. It is predicting the people who will not get sick with 96% accuracy while the sick are spreading the virus!
>
> Do you think this is a correct metric for our model given the seriousness of the issue? Shouldn’t we be measuring how many positive cases we can predict correctly to arrest the spread of the contagious virus? Or maybe, out of the correctly predicted cases, how many are positive cases to check the reliability of our model?
>
> This is where we come across the dual concept of Precision and Recall.
>
> Precision tells us how many of the correctly predicted cases actually turned out to be positive.
>
> Recall tells us how many of the actual positive cases we were able to predict correctly with our model.
>
> Precision is a useful metric in cases where False Positive is a higher concern than False Negatives.
>
> Recall is a useful metric in cases where False Negative trumps False Positive.
>
> Recall is important in medical cases where it doesn’t matter whether we raise a false alarm but the actual positive cases should not go undetected!

[This is a lifesaver](https://www.tablesgenerator.com/) when making LaTeX or Markdown or HTML tables

---

[Max Tegmark on Lex Fridman's podcast](https://www.youtube.com/watch?v=dinfiuGqoQw) talking about whether AI can discover new laws of Physics. I had a burning, inchoate question about whether formulas could be 'discovered' and he mentions Symbolic Regression. This is, of course, pretty easy with a _Linear_ formulation. But it's very hard when you start involving other symbols: sine, cosine, etc. I thought that it was an NP-hard problem and was happy to [find out that I was right](https://en.wikipedia.org/wiki/Symbolic_regression).

Regression is a _problem_ and _Linear_ Regression is a way to _solve_ the problem. It's not the only one.

### Jupyter stuff

https://stackoverflow.com/a/61985137

```
jupyter kernelspec list
jupyter kernelspec uninstall <name of kernel>
```

Dimensionality Reduction: You want to discover the "latent dimensions along which the data varies".

---

[Cheaper Diploma Frame](https://www.amazon.com/gp/product/B08TP9Z8WF/)

[Deep Learning course from Andrew Ng](https://www.deeplearning.ai/courses/ai-for-medicine-specialization/)

[Andrej Karpathy - Deep Learning Zero to Hero](https://karpathy.ai/zero-to-hero.html)

[GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/)
